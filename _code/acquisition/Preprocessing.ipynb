{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=center> Preprocessing </h2>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Convert Datetime](#convert-datetime)\n",
    "- [Document Classification](#document-classification)\n",
    "- [Text Preprocessing] (#text-preprocessing)\n",
    "    - [Tokenization] (#tokenization)\n",
    "    - [Remove special characters] (#remove-special-characters)\n",
    "    - [Stemming & Lemmatization] (#stemming-lemming)\n",
    "    - [Removing Stopwords] (#remove-stop-words)\n",
    "- [Text Preprocessing: A Function](#text-preprocessing-function)\n",
    "- [Target Variable Numeric Encoding](#target-variable-numeric-encoding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from num2words import num2words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/rashidbaset/Code/cap_project/_data/raw-data/Tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_created'] = pd.to_datetime(df['tweet_created'], format = \"%Y-%m-%d %H :%M:%S\", errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking to first classify tweet as either neutral or non-neutral sentiment, then classify sentiment in tweets that are predicted to have polarity to simplify analysis to consider only positive and negative tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping tweets that were classified with full confidence by labelers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['airline_sentiment']!='neutral']\n",
    "df = df[df['airline_sentiment_confidence']==1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This narrowed our dataset to 8897 observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8897"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text analysis we're interested in processing text data to convert them into something coherent for analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We followed 4 steps:\n",
    "\n",
    "1. Tokenization \n",
    "2. Remove special characters \n",
    "3. Stemming & Lemmatization\n",
    "4. Removing Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting texts into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain a better understanding of what's happening under the hood when tokenizing, we pick some sentences that we're interested in comparing. The list will be used to compare the performance between different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n14631  569588464896876545          negative                        1.0000   \n14633  569587705937600512          negative                        1.0000   \n14634  569587691626622976          negative                        0.6684   \n14636  569587371693355008          negative                        1.0000   \n14638  569587188687634433          negative                        1.0000   \n\n               negativereason  negativereason_confidence   airline  \\\n14631              Bad Flight                     1.0000  American   \n14633        Cancelled Flight                     1.0000  American   \n14634             Late Flight                     0.6684  American   \n14636  Customer Service Issue                     1.0000  American   \n14638  Customer Service Issue                     0.6659  American   \n\n      airline_sentiment_gold             name negativereason_gold  \\\n14631                    NaN         MDDavis7                 NaN   \n14633                    NaN  RussellsWriting                 NaN   \n14634                    NaN    GolfWithWoody                 NaN   \n14636                    NaN         itsropes                 NaN   \n14638                    NaN       SraJackson                 NaN   \n\n       retweet_count                                               text  \\\n14631              0  @AmericanAir thx for nothing on getting us out...   \n14633              0  @AmericanAir my flight was Cancelled Flightled...   \n14634              0         @AmericanAir right on cue with the delays👌   \n14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n14638              0  @AmericanAir you have my money, you change my ...   \n\n      tweet_coord              tweet_created tweet_location  \\\n14631         NaN  2015-02-22 12:04:07 -0800             US   \n14633         NaN  2015-02-22 12:01:06 -0800    Los Angeles   \n14634         NaN  2015-02-22 12:01:02 -0800            NaN   \n14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n\n                    user_timezone  \n14631  Eastern Time (US & Canada)  \n14633                     Arizona  \n14634                       Quito  \n14636                         NaN  \n14638  Eastern Time (US & Canada)  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>14631</th>\n      <td>569588464896876545</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>1.0000</td>\n      <td>American</td>\n      <td>NaN</td>\n      <td>MDDavis7</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@AmericanAir thx for nothing on getting us out...</td>\n      <td>NaN</td>\n      <td>2015-02-22 12:04:07 -0800</td>\n      <td>US</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>14633</th>\n      <td>569587705937600512</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Cancelled Flight</td>\n      <td>1.0000</td>\n      <td>American</td>\n      <td>NaN</td>\n      <td>RussellsWriting</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@AmericanAir my flight was Cancelled Flightled...</td>\n      <td>NaN</td>\n      <td>2015-02-22 12:01:06 -0800</td>\n      <td>Los Angeles</td>\n      <td>Arizona</td>\n    </tr>\n    <tr>\n      <th>14634</th>\n      <td>569587691626622976</td>\n      <td>negative</td>\n      <td>0.6684</td>\n      <td>Late Flight</td>\n      <td>0.6684</td>\n      <td>American</td>\n      <td>NaN</td>\n      <td>GolfWithWoody</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@AmericanAir right on cue with the delays👌</td>\n      <td>NaN</td>\n      <td>2015-02-22 12:01:02 -0800</td>\n      <td>NaN</td>\n      <td>Quito</td>\n    </tr>\n    <tr>\n      <th>14636</th>\n      <td>569587371693355008</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Customer Service Issue</td>\n      <td>1.0000</td>\n      <td>American</td>\n      <td>NaN</td>\n      <td>itsropes</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n      <td>NaN</td>\n      <td>2015-02-22 11:59:46 -0800</td>\n      <td>Texas</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14638</th>\n      <td>569587188687634433</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Customer Service Issue</td>\n      <td>0.6659</td>\n      <td>American</td>\n      <td>NaN</td>\n      <td>SraJackson</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@AmericanAir you have my money, you change my ...</td>\n      <td>NaN</td>\n      <td>2015-02-22 11:59:02 -0800</td>\n      <td>New Jersey</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "df[df['airline_sentiment'] == 'negative'].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_list = ['@united stuck here in IAH waiting on flight 253 to Honolulu for 7 hours due to maintenance issues. Could we have gotten a new plane!?!? Fail',\n",
    "               '@JetBlue had a great flight to Orlando from Hartford a few weeks ago! Was great to get out on time and arrive early!',\n",
    "               '@AmericanAir my flight was Cancelled Flightled, leaving tomorrow morning. Auto rebooked for a Tuesday night flight but need to arrive Monday.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original:\n@JetBlue had a great flight to Orlando from Hartford a few weeks ago! Was great to get out on time and arrive early!\n\nTokenized:\n['@JetBlue', 'had', 'a', 'great', 'flight', 'to', 'Orlando', 'from', 'Hartford', 'a', 'few', 'weeks', 'ago', '!', 'Was', 'great', 'to', 'get', 'out', 'on', 'time', 'and', 'arrive', 'early', '!']\n"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet=df.loc[8644, 'text'] \n",
    "Tokenizer = TweetTokenizer()\n",
    "tokenized = Tokenizer.tokenize(tweet)\n",
    "\n",
    "print('Original:')\n",
    "print(tweet)\n",
    "print('\\nTokenized:')\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With TweetTokenizer, we're using a tokenizer built to tokenize tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuation and converting characters to lowercase. The eclamation mark may be informative about the sentiment, so keep this as a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['@jetblue', 'had', 'a', 'great', 'flight', 'to', 'orlando', 'from', 'hartford', 'a', 'few', 'weeks', 'ago', '!', 'was', 'great', 'to', 'get', 'out', 'on', 'time', 'and', 'arrive', 'early', '!']\n"
    }
   ],
   "source": [
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "punctuation.remove('!')\n",
    "tokenized_no_punctuation=[word.lower() for word in tokenized if word not in punctuation]\n",
    "print(tokenized_no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['@jetblue', 'great', 'flight', 'orlando', 'hartford', 'weeks', 'ago', '!', 'great', 'get', 'time', 'arrive', 'early', '!']\n"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "tokenized_no_stopwords=[word for word in tokenized_no_punctuation if word not in stopwords.words('english')]\n",
    "print(tokenized_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We choose the PorterStemmer library for stemming and lemmatization from the NLTK package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['@jetblu', 'great', 'flight', 'orlando', 'hartford', 'week', 'ago', '!', 'great', 'get', 'time', 'arriv', 'earli', '!']\n"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "tokens = [PorterStemmer().stem(word) for word in tokenized_no_stopwords]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing it all together: A function to apply tweets to create data column containing tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "def tweet_preprocessor(text):\n",
    "    tokenized = Tokenizer.tokenize(text)\n",
    "    punctuation = list(string.punctuation)\n",
    "    punctuation.remove('!')\n",
    "    tokenized_no_punctuation=[word.lower() for word in tokenized if word not in punctuation]\n",
    "    tokenized_no_stopwords=[word for word in tokenized_no_punctuation if word not in stopwords.words('english')]\n",
    "    tokens = [PorterStemmer().stem(word) for word in tokenized_no_stopwords if word != '️']\n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            tokens[i]=num2words(tokens[i])\n",
    "        except:\n",
    "            pass\n",
    "    return tokens\n",
    "\n",
    "# Applies the tweet_preprocessor function separately to each element of the column 'message' \n",
    "df['tokens']=df['text'].apply(tweet_preprocessor)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taking a look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                                 text  \\\n3   @VirginAmerica it's really aggressive to blast...   \n4   @VirginAmerica and it's a really big bad thing...   \n5   @VirginAmerica seriously would pay $30 a fligh...   \n9   @VirginAmerica it was amazing, and arrived an ...   \n11  @VirginAmerica I &lt;3 pretty graphics. so muc...   \n12  @VirginAmerica This is such a great deal! Alre...   \n14                             @VirginAmerica Thanks!   \n16  @VirginAmerica So excited for my first cross c...   \n17  @VirginAmerica  I flew from NYC to SFO last we...   \n18                    I ❤️ flying @VirginAmerica. ☺️👍   \n\n                                               tokens  \n3   [@virginamerica, realli, aggress, blast, obnox...  \n4           [@virginamerica, realli, big, bad, thing]  \n5   [@virginamerica, serious, would, pay, thirty, ...  \n9    [@virginamerica, amaz, arriv, hour, earli, good]  \n11  [@virginamerica, <3, pretti, graphic, much, be...  \n12  [@virginamerica, great, deal, !, alreadi, thin...  \n14                         [@virginamerica, thank, !]  \n16  [@virginamerica, excit, first, cross, countri,...  \n17  [@virginamerica, flew, nyc, sfo, last, week, f...  \n18                     [❤, fli, @virginamerica, ☺, 👍]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>tokens</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>[@virginamerica, realli, aggress, blast, obnox...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>[@virginamerica, realli, big, bad, thing]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n      <td>[@virginamerica, serious, would, pay, thirty, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n      <td>[@virginamerica, amaz, arriv, hour, earli, good]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n      <td>[@virginamerica, &lt;3, pretti, graphic, much, be...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>@VirginAmerica This is such a great deal! Alre...</td>\n      <td>[@virginamerica, great, deal, !, alreadi, thin...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>@VirginAmerica Thanks!</td>\n      <td>[@virginamerica, thank, !]</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>@VirginAmerica So excited for my first cross c...</td>\n      <td>[@virginamerica, excit, first, cross, countri,...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>@VirginAmerica  I flew from NYC to SFO last we...</td>\n      <td>[@virginamerica, flew, nyc, sfo, last, week, f...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>I ❤️ flying @VirginAmerica. ☺️👍</td>\n      <td>[❤, fli, @virginamerica, ☺, 👍]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df[['text','tokens']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding target variable numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              tweet_id airline_sentiment  airline_sentiment_confidence  \\\n3   570301031407624196          negative                           1.0   \n4   570300817074462722          negative                           1.0   \n5   570300767074181121          negative                           1.0   \n9   570295459631263746          positive                           1.0   \n11  570289724453216256          positive                           1.0   \n\n   negativereason  negativereason_confidence         airline  \\\n3      Bad Flight                     0.7033  Virgin America   \n4      Can't Tell                     1.0000  Virgin America   \n5      Can't Tell                     0.6842  Virgin America   \n9             NaN                        NaN  Virgin America   \n11            NaN                        NaN  Virgin America   \n\n   airline_sentiment_gold          name negativereason_gold  retweet_count  \\\n3                     NaN      jnardino                 NaN              0   \n4                     NaN      jnardino                 NaN              0   \n5                     NaN      jnardino                 NaN              0   \n9                     NaN    YupitsTate                 NaN              0   \n11                    NaN  HyperCamiLax                 NaN              0   \n\n                                                 text tweet_coord  \\\n3   @VirginAmerica it's really aggressive to blast...         NaN   \n4   @VirginAmerica and it's a really big bad thing...         NaN   \n5   @VirginAmerica seriously would pay $30 a fligh...         NaN   \n9   @VirginAmerica it was amazing, and arrived an ...         NaN   \n11  @VirginAmerica I &lt;3 pretty graphics. so muc...         NaN   \n\n                tweet_created tweet_location               user_timezone  \\\n3   2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n4   2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n5   2015-02-24 11:14:33 -0800            NaN  Pacific Time (US & Canada)   \n9   2015-02-24 10:53:27 -0800    Los Angeles  Eastern Time (US & Canada)   \n11  2015-02-24 10:30:40 -0800            NYC            America/New_York   \n\n                                               tokens  positive  \n3   [@virginamerica, realli, aggress, blast, obnox...         0  \n4           [@virginamerica, realli, big, bad, thing]         0  \n5   [@virginamerica, serious, would, pay, thirty, ...         0  \n9    [@virginamerica, amaz, arriv, hour, earli, good]         1  \n11  [@virginamerica, <3, pretti, graphic, much, be...         1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n      <th>tokens</th>\n      <th>positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n      <td>[@virginamerica, realli, aggress, blast, obnox...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n      <td>[@virginamerica, realli, big, bad, thing]</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>570300767074181121</td>\n      <td>negative</td>\n      <td>1.0</td>\n      <td>Can't Tell</td>\n      <td>0.6842</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:33 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n      <td>[@virginamerica, serious, would, pay, thirty, ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>570295459631263746</td>\n      <td>positive</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>YupitsTate</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it was amazing, and arrived an ...</td>\n      <td>NaN</td>\n      <td>2015-02-24 10:53:27 -0800</td>\n      <td>Los Angeles</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n      <td>[@virginamerica, amaz, arriv, hour, earli, good]</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>570289724453216256</td>\n      <td>positive</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>HyperCamiLax</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I &amp;lt;3 pretty graphics. so muc...</td>\n      <td>NaN</td>\n      <td>2015-02-24 10:30:40 -0800</td>\n      <td>NYC</td>\n      <td>America/New_York</td>\n      <td>[@virginamerica, &lt;3, pretti, graphic, much, be...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df['positive']=(df['airline_sentiment']=='positive').astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving work and only keeping columns which we will use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['airline_sentiment', 'airline', 'tokens', 'positive', 'text', 'negativereason']]\n",
    "pd.DataFrame(df).to_csv('/Users/rashidbaset/Code/cap_project/_data/processed/text_processed.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}